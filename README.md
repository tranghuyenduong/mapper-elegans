# mapper-elegans
First create empty data folder that has 5 empty folders name: reads, alignments, ws253, refs, tmp
You will need to download ws253 genome in .fa format, gff2, gff3, gtf files from wormbase. The gtf file on wormbase might need some minor modification before but you would be able to identify the "error" once running thr "ref.py", then modify according to the instruction. 
I do have the "ready to go" gtf, ws253.fa, gff2, gff3 files if you want to be transfer it to you via external hardrive, since github doenst accept data transfering
Now you can run ref.py: the empty folder will be filled up with data once you run ref.py. 
One you finished running refs.py you should get a data set from GEO or any small RNA data in .fa format, deposite this data set into "reads" folder in "data" folder
identify the barcode of the sequencing data set and change it acordingly in setting.py: template = "AGATCGGAAGAGCACACGTCTGAACTCC"
Then run map.py
The alignment data should be populated in the mapper_elegans folder as 8 different folder: 4 for finnal alignment result which included: intron_inter, intron_exon, exon_exon, exon, and 4 for unfilter of miRNA, piRNA result which included: unfilter...




Here is the extra information that explain the detail of each steps
Pipeline structure: refs.py, map.py, settings and modules.
The only two “packages” are refs.py and map.py. “Refs.py” is the first step, with the purpose is to generating reference files for genome for mapping. The input is the latest annotated genome from wormbase, which in our case is WS253 was used as the reference genome. The “class RefsConfig” is in “settings.py”, and the functions that were called in “refs.py” are in “_init_.py-modules”. The output of refs.py is refs file. 
“Map.py” is the mapping step, which included of 6 small steps: 1. Pre-processing, 2. Generating formatted Bowtie alignment records and aligning, 3. Classification alignments, 4. Post-processing alignments, 5. Extract gene intersections, 6. Filter alignments.
1. Pre-processing: 
We custom our pipeline so that it take fastq/fasta file as the input. The small RNA libraries we included in this study are 11 libraries that our laboratory sequenced through illumina are available in fastq format. However, most of the public available sequencing data from GEO are in sra format. To process the data from GEO, we first need to download those into your server/computer in sra format using FTP download site available on GEO. Then we used sratool kit to convert those data set into fastq/fasta format before feeding in our pipeline using the command line “fastq-dump --split-3 SRR5079708.sra” (example). 
The very first step in the pipeline is pre-processing, which included trimmed barcode, clipped adapter, and collapsed reads (modules->Preprocessor). Some of the libraries have the barcode trimmed or separated from the sequence (our data), those we manually changed “trim_barcode = false” in PreprocessConfig (settings.py) to skip the trimming process. Otherwise, the barcodes are trimmed from 5’ end using fastx-trimmer, with the following parameters: -f, -Q33, -i, -o (settings.py). Then the adapters were clipped using fastx-clipper with the following parameters: -a, -c, -M, -l , -v, -Q33, -I, -o. In this step, we customized the pipeline so that the raw data is chunked into small part (in other words the 4 lines of code that represent each read are clumped into one unit that is then fed into the core as a single entity) to feed into each core of the multicore processor: each record is 4 lines, there the total number of record = total number of lines divide by 4. Then assign evenly the number of record to each core = total number of record/12. These records ar e then clipped of adaptor sequences using fastx-clipper. Only reads with adapter sequence clipped off are kept for alignment (parameter in fast-X clipper). After clipping completed, combined the results as a single output file, “clipped-fq”, in a temporary file for future use (mapper-elegans/data/tmp). To identify the adaptor sequence for each library, we manually scan through 5-10 reads looking for repeated sequences, then do a web search for a full length of that adaptor sequence. This full length adaptor sequence is the template for adaptor (settings/preprocessor/template=). Then we collapsed the reads into unique reads. We wrote a function in python (modules/preprocessor/_init_.py/collapse_reads) to identify unique reads and to number unique reads. This function also allows us to ignore reads longer than 27 (can change in setting.py/ PreprocessConfig) and shorter than 17 (can change in setting.py/ PreprocessConfig). The output file is “collapsed.fq”, which contains 2 line of information for each read. The first line (>15-25-1) has the identification number of the read (15) followed by the length of the read (25) then the copy number of the read (1). The second line is sequence of the read. Example below: 
>15-25-1
AATTTCCGGCAATTTCCGATTTGCA
The collapsed.fq file is also saved in data/tmp file. 

2. Generating formatted Bowtie alignment records and aligning. 
a. To have only anti-sense reads mapping to the genome, we reformat the reference genome to have a new Bowtie alignment records. To do this I first wrote a python script call “RecordFomatter” in “ReadAligner” module to identify all the genes that are annotated as being transcribed from the negative strand (-) (WS253, refs, transcript_bed). Second, the script created the reverse complement sequence of each gene that was identified as being transcribed from the (-) strand. Third it replaced the all the genes that are annotated as being transcribed from negative strand (-) with the corresponding reverse complement. These genes were reannotated as -1. This new reference genome is generated by using refs.py. The “class RefsConfig” is in “settings.py”, and the functions that were called in “refs.py” are in “_init_.py-modules”. The output of refs.py is stored in refs file  “data/refs/bt/genome”. This annotated genome will be used to align the small RNA library.
b. In “ReadAligner” script we used Bowtie-2 as our aligner, with the following parameters: -f, -v 0, --all, --best, --strata (find explanation in bowtie-2 manual). We aligned with both genome and cds reference for classification purposes. The output files are “genome_alignments” and “cds_alignments” in both SAM and Text format. 

3. Classification alignments: In “SourceFinder” in modules, we assign the reads into four different classifiers (a-d): a intron, b exon, c intron-exon, d exon-exon.  
To identify reads that map to introns or intron_exon junctions (I designated this category “iiie”) we took all the reads that mapped to the genome and subtracted from these the reads that mapped to the coding sequence: intron_or_intergenic_or_intron_exon_alignments = (genome_alignments) – (cds_alignments).
Note: 5’ and 3’UTRs are included the annotated coding region and thus are excluded from iiie.
Classifier c: to identify reads that mapped to intron_exon intersection, I fed (iiie) through bedtools program with the following parameters: intersect, -wo, -S, -a, stdin, -b (the bedtools manual explains the parameters - https://bedtools.readthedocs.io/en/latest/content/tools/intersect.html). The output, “intron_exon_alignments”, is formatted with chrom (chromosome), chrom_start, chrom_end, strand information, and it classifier (intron_exon). 
Classifier a: to identify reads that map to introns, I took iiie minus intron_exon (classifier c).
Classifier d: to identify reads that map to exon_exon junctions, I took all the reads that map to “cds_alignments” then excluded the same reads from “genome_alignments”:  “exon_exon_alignments” = “cds_alignments” – “genome_alignments”
Classifier b: Reads are classified as “Exon” are reads that mapped to cds sequence, “cds_alignment” minus “exon_exon alignments”.
All the reads after classification are tagged with the classification information: intron, intron-exon, exon, exon-exon and written into “classify_all”, which is defined in step 3 in “map.py”.  

4. Post-processing alignments
In “Postprocessor” module we have 4 functions: “count_mapped_loci”, “correct_read_counts”, “find_pirna_mirna_reads”, and “process_alignments”. In “count_mapped_loci”, we count number of unique loci for each library. Then in “correct_read_counts”, we count and record the number of reads (score) for each of the unique loci identified above. In “find_pirna_mirna_reads”, we used pirna_mirna_records = P("refs/pirna_mirna") that defined as “PostprocessConfig” function in settings.py. In order to identify to find the reads that overlapped with “pirna_mirna_records”, we used “bedtools” with following parameters: intersect, -wao, -f, 1.0, -s, -a, stdin, -b. This information is updated into “classify_all” file.

5. Extract gene intersections and filtering the pirna, mirna, multimapped alignments.
The main purpose in “extract gene intersections” step is to identify the gene name for the loci that the reads mapped to. 

6. Filter alignments added the filter for filtering out snRNA, snoRNA, tRNA
